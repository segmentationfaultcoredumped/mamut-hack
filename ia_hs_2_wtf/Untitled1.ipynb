{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./plabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!! As a woman you shouldn't complain about cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! boy dats cold...tyga dwn bad for cuffin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!! Dawg!!!! You ever fuck a bitch and she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!! she look like a tranny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!! The shit you hear about me might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"The shit just blows me..cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!!!\"I can not just sit up and HATE on anoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!;cause I'm tired of you big bitches coming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>\" &amp;amp; you might not get ya bitch back &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>\" :hobbies include: fighting Mariam\"\\n\\nbitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Keeks is a bitch she curves everyone \" lol I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Murda Gang bitch its Gang Land \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>\" So hoes that smoke are losers ? \" yea ... go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bad bitches is the only thing that i like \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bitch get up off me \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bitch nigga miss me with it \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bitch plz whatever \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bitch who do you love \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>\" bitches get cut off everyday B \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>\" black bottle &amp;amp; a bad bitch \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>\" broke bitch cant tell me nothing \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>\" cancel that bitch like Nino \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>\" cant you see these hoes wont change \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>\" fuck no that bitch dont even suck dick \" ;;;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>\" got ya bitch tip toeing on my hardwood floor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>\" her pussy lips like Heaven doors \" ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>\" hoe what its hitting for \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>\" i met that pussy on Ocean Dr . i gave that p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>\" i need a trippy bitch who fuck on Hennessy \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>\" i spend my money how i want bitch its my bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>\" i txt my old bitch my new bitch pussy wetter \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>\" i'd say im back to the old me but my old bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>\" if you aint bout that Murder Game pussy nigg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>\" if you're toes ain't done you pussy stinks \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>\" im done with bitter bitches its a wrap for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>\" is that ya bitch \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>\" it aint nothing to cut a bitch off \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>\" jus meet son now he ya mane ass bitches \" #S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>\" lames crying over hoes thats tears of a clown \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>\" like Snoop said in 94 we dont love these hoes \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>\" momma said no pussy cats inside my doghouse \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>\" most hated but the hoes favorite \" #2MW #Sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>\" nice girls bad, make me get naughty. Bad yel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>\" pimps up pimps up hoes down \" Future voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>\" post a picture of that pussy get 200 likes \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>\" pussy is a powerful drug \" ; #HappyHumpDay h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>\" quick piece of pussy call it a drive by \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>\" running round here like some brand new pussy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>\" these bitches even worst they'll send them g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>\" these hoes like niggas that spend money not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet\n",
       "0      1  !!! As a woman you shouldn't complain about cl...\n",
       "1      0  !!!!! boy dats cold...tyga dwn bad for cuffin ...\n",
       "2      0  !!!!!!! Dawg!!!! You ever fuck a bitch and she...\n",
       "3      0                   !!!!!!!!! she look like a tranny\n",
       "4      0  !!!!!!!!!!!!! The shit you hear about me might...\n",
       "5      0  !!!!!!!!!!!!!!!!!!\"The shit just blows me..cla...\n",
       "6      0  !!!!!!\"I can not just sit up and HATE on anoth...\n",
       "7      0  !!!!;cause I'm tired of you big bitches coming...\n",
       "8      0  \" &amp; you might not get ya bitch back &amp; ...\n",
       "9      0      \" :hobbies include: fighting Mariam\"\\n\\nbitch\n",
       "10     0  \" Keeks is a bitch she curves everyone \" lol I...\n",
       "11     0                 \" Murda Gang bitch its Gang Land \"\n",
       "12     0  \" So hoes that smoke are losers ? \" yea ... go...\n",
       "13     0      \" bad bitches is the only thing that i like \"\n",
       "14     0                            \" bitch get up off me \"\n",
       "15     0                    \" bitch nigga miss me with it \"\n",
       "16     0                             \" bitch plz whatever \"\n",
       "17     0                          \" bitch who do you love \"\n",
       "18     0                 \" bitches get cut off everyday B \"\n",
       "19     0                 \" black bottle &amp; a bad bitch \"\n",
       "20     0               \" broke bitch cant tell me nothing \"\n",
       "21     0                    \" cancel that bitch like Nino \"\n",
       "22     0            \" cant you see these hoes wont change \"\n",
       "23     0  \" fuck no that bitch dont even suck dick \" ;;;...\n",
       "24     0  \" got ya bitch tip toeing on my hardwood floor...\n",
       "25     0             \" her pussy lips like Heaven doors \" ;\n",
       "26     0                       \" hoe what its hitting for \"\n",
       "27     0  \" i met that pussy on Ocean Dr . i gave that p...\n",
       "28     0     \" i need a trippy bitch who fuck on Hennessy \"\n",
       "29     0  \" i spend my money how i want bitch its my bus...\n",
       "30     0   \" i txt my old bitch my new bitch pussy wetter \"\n",
       "31     0  \" i'd say im back to the old me but my old bit...\n",
       "32     0  \" if you aint bout that Murder Game pussy nigg...\n",
       "33     0     \" if you're toes ain't done you pussy stinks \"\n",
       "34     0  \" im done with bitter bitches its a wrap for t...\n",
       "35     0                               \" is that ya bitch \"\n",
       "36     0             \" it aint nothing to cut a bitch off \"\n",
       "37     0  \" jus meet son now he ya mane ass bitches \" #S...\n",
       "38     0  \" lames crying over hoes thats tears of a clown \"\n",
       "39     0  \" like Snoop said in 94 we dont love these hoes \"\n",
       "40     1    \" momma said no pussy cats inside my doghouse \"\n",
       "41     0  \" most hated but the hoes favorite \" #2MW #Sev...\n",
       "42     0  \" nice girls bad, make me get naughty. Bad yel...\n",
       "43     0       \" pimps up pimps up hoes down \" Future voice\n",
       "44     0     \" post a picture of that pussy get 200 likes \"\n",
       "45     0  \" pussy is a powerful drug \" ; #HappyHumpDay h...\n",
       "46     0        \" quick piece of pussy call it a drive by \"\n",
       "47     0  \" running round here like some brand new pussy...\n",
       "48     0  \" these bitches even worst they'll send them g...\n",
       "49     0  \" these hoes like niggas that spend money not ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31348,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['tweet'], data['class'], test_size=0.3, random_state=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-96050de9f6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/mamut/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/mamut/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/mamut/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/mamut/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    144\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(x_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names â€˜vectâ€™ , â€˜tfidfâ€™ and â€˜clfâ€™ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Support Vector Machines - SVM and calculating its performance\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(x_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(x_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
    "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
    "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
    "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=5)\n",
    "gs_clf = gs_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the best mean score and the params, run the following code\n",
    "\n",
    "gs_clf.best_params_\n",
    "gs_clf.best_score_\n",
    "\n",
    "# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! ðŸ˜„)\n",
    "# and the corresponding parameters are {â€˜clf__alphaâ€™: 0.01, â€˜tfidf__use_idfâ€™: True, â€˜vect__ngram_rangeâ€™: (1, 2)}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_gs_clf = gs_clf.predict(x_test)\n",
    "\n",
    "np.mean(predicted_gs_clf == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly doing grid search for SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1, cv=5)\n",
    "gs_clf_svm = gs_clf_svm.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_gs_clf_svm = gs_clf_svm.predict(x_test)\n",
    "\n",
    "np.mean(predicted_gs_clf_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "# Removing stop words\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
    "                     ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming Code\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(x_train, y_train)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(x_test)\n",
    "\n",
    "np.mean(predicted_mnb_stemmed == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_svm_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=10, random_state=42))])\n",
    "\n",
    "text_svm_mnb_stemmed = text_svm_mnb_stemmed.fit(x_train, y_train)\n",
    "\n",
    "predicted_svm_mnb_stemmed = text_svm_mnb_stemmed.predict(x_test)\n",
    "\n",
    "np.mean(predicted_svm_mnb_stemmed == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20620 - 4163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_svm_mnb_stemmed.predict([\"!!!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "jb.dump(text_svm_mnb_stemmed, 'clf.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
